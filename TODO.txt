TODO (maybe):

-explore covariance (or other metrics) between observed and expected rewards to see if the network is learning this (even when network.certainty ~= 0)
-consider programming in a resignation threshold to speed up training
-add estimated time remaining when training/generating data
-More visualizations to better understand network progress:
	-Euclidean distance of current weight vector relative to initial vector
	-Save some diverse set of examples, and graph activation sparsity on the set by layer
	-Top N lowest and N highest cost examples from a large data set (graph expected vs. recevied rewards)
	-take a snapshot of the weight vector at some time, find the top 2 PCs/ eigenvectors (?), and graph a time
	 series of points where the 2 axes of the graph are the PCs (each point is another snapshot of the weights)
-Consider recording 'certainty' on some interval so it can be graphed vs. number of training steps or unique examples
-add computational time as Network attribute (to supplement 'age' and 'experience')

Misc thoughts/worries:

-while the architecture related to the policy output is efficient in terms of using a low number of learnable parameters, information about the true probability distribution over legal moves is lossily compressed in a way which may be problematic, especially in particular positions. In reality, certain start sqaures are associated only with particular end squares and particular end pieces. However, these associations are totally lost (they are computed independently) with the current architecture
-I've seen the engine select bad moves that appeared good simply because of limited search depth (e.g. the final move/ edge in the search tree is a rewarding capture which would then be recaptured with equal or greater reward were the tree to cover that far). Setting "gamme_exec" high exacerbates this problem, yet setting it low leads to reckless material grabbing. Note that this problem goes completely away in the limit that the network's value output becomes perfectly accurate, regardless of "gamma_exec". However, the problem is highly noticeable when "gamma_exec"=1 even when value certainty ~= 0.5.

Verification:

-verify that choice of hyperparameters (excluding network arhcitecture) is reasonable to minimize loss on tf-starter datasets
-sample tf-starter datasets randomly and observe by eye that examples look reasonable
-(overall approach and program integrity) verify that a network can learn from an external chess dataset
-buffer.combine has no unit test

Eventually:

-allow the network freedom to parameterize its own training: namely at least having it compute some metric rating its competency, and weight the relative values of material to wins/losses accordingly.
