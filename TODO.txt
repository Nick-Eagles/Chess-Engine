TODO:

-momentum terms should be saved with Network to avoid sudden halting of momentum when saving and restarting
 program

TODO (maybe):

-explore covariance (or other metrics) between observed and expected rewards to see if the network is learning this
 (even when network.certainty ~= 0)
-maybe implement "autosaving" on some interval to avoid multiprocessing getting stuck
-consider programming in a resignation threshold to speed up training
-implement alpha-beta-like pruning to greatly optimize tree search
-add estimated time remaining when training/generating data
-More visualizations to better understand network progress:
	-Euclidean distance of current weight vector relative to initial vector
	-Save some diverse set of examples, and graph activation sparsity on the set by layer
	-Top N lowest and N highest cost examples from a large data set (graph expected vs. recevied rewards)
	-take a snapshot of the weight vector at some time, find the top 2 PCs/ eigenvectors (?), and graph a time
	 series of points where the 2 axes of the graph are the PCs (each point is another snapshot of the weights)
-Consider recording 'certainty' on some interval so it can be graphed vs. number of training steps or unique examples
-add computational time as Network attribute (to supplement 'age' and 'experience')

Misc thoughts/worries:

-does having a minimum certainty threshold (>0) for computing NN evals in policy.getEvals make more sense?
-currently, when certainty <= 0 and there are no captures available, the policy selects the first legal move
 computed (this is certainly not rational and could hurt exploration- the move should be randomly selected)

Eventually:

-allow the network freedom to parameterize its own training: namely at least having it
 compute some metric rating its competency, and weight the relative values of material to
 wins/losses accordingly.
